1. Convolutional Neural Network can be trained mainly as a supervised learning.
	a) Áno
	b) Nie
	  	
2. Among the following, which one are hyperparameters
	a) number of layers in the neural network
	b) learning rate α (alpha)
	c) size(s) of hidden layers
	d) number of iterations
	
3. Artificial neural networks are loosely inspired by biological neural networks
	a) Áno
	b) Nie
	
4. Which of the following about Reccurent Neural Networks (RNN) is True?
	a) RNN can be used for machine translation, for example, from English to French because 
	b) It looks like CRISP-DM iterative recurrent process 
	c) It is suitable for sequence data
	
5. Neural networks are not intended to be realistic models of the brain
	a) Áno
	b) Nie
	
6. Neural networks are robust algorithms and data structures able to model difficult problems.
	a) Áno
	b) Nie)
	
7. Assign to the correct group (Singular Value Decomposition / Distributional Semantics): - SVD: , Distributional Semantics:
	a) Truncated SVD
	b) Word Embeddings 
	c) CBOW
	d) Latent Semantic Indexing (LSI)
	e) Skip-Gram
	f) Latent Semantic Analysis (LSA)
	
8. Which of following is a vector representation of text?
	a) n-grams
	b) autoencoding
	c) approximated function
	d) bag-of-words
	e) TF-IDF
	f) word embedding
	
9. Assign to the correct group (Euclidian distance / Cosine similarity)
	a) the distance between two points
	b) the angle between two vectors
	
10. In Neural Language processing (NLP), stemming is a technique:
	a) to map valid words to root and leaves
	b) to split words, phrases, idioms
	c) to discover topics in a collection of documents
	d) to determine where one word ends and another begins
	
11. Which of the following is an important component of the text processing pipeline?
	a) tfidfTransformer
	b) CountVectorizer
	c) Regression estimator
	
12. Vyberte iba jednu z možností:
	a = numpy.array(
	[[30, 65, 70],
	[80, 95, 10],
	[50, 90, 60]])
	calculate max(a, axis=1)
	a) another value
	b) [70, 95, 90]
	c) [80, 95, 70]
	d) 95
	
13. What will the code print?
	arr2d = numpy.arange(0, 30)
	arr2d = arr2d.reshape([3, 10])
	print(arr2d[:,0])
	a) [0 1 2 3 4 5 6 7 8 9]
	b) [0 10 20]
	
14. What will the code print?
	students = ['eva', 'milan', 'bob', 'marek', 'lenka', 'peter']
	print(students[3:5])
	a) ['marek', 'lenka']
	b) ['bob', 'marek']
	c) ['lenka', 'peter']
	d) bad code, it doesn't work
	
15. Is the following slicing technique right?
	t = (2, 5, 7, 9, 10, 11, 12)
	t[2:4]
	a) Áno
	b) Nie
	
16. Vyberte iba jednu možnosť:
	a = numpy.array(
	[[30, 65, 70],
	[80, 95, 10],
	[50, 90, 60]])
	Calculate median(a, axis=0)
	a) 65
	b) [50, 90, 60]
	c) [65, 80, 60]
	d) another values

17. Vyberte iba jednu možnosť:
	a = numpy.array(
	[[30, 65, 70],
	[80, 95, 10],
	[50, 90, 60]])
	Calculate mean(a, axis=0)
	a) [53.3333, 83.333, 46.667]
	b) 61.111
	c) another values
	d) [55.000, 61.667, 66.667]
	
18. Vyberte iba jednu možnosť:
	a = numpy.array(
	[[30, 65, 70],
	[80, 95, 10],
	[50, 90, 60]])
	Calculate numpy.max(a, axis=0)
	a) another values
	b) [70, 95, 90]
	c) 95
	d) [80, 95, 70]
	
19. Suppose that your linear regression model is underfitting the data. Which of the following regularizations would you consider?
	a) L2
	b) L1 and L2
	c) L1
	d) None of these
	
20. Suppose that you have dataset D and the perfect fit polynomial regression model of degree 3. What will happen when you fit polynomial regression of degree 2?
	a) There are high changes of overfitting
	b) Nothing
	c) There are high chances of underfitting
	
21. Linear regression is mainly used for Regression.
	a) Áno
	b) Nie
	
22. Regularization is used in case of underfitting
	a) Áno
	b) Nie
	
23. Regularization is used in case of overfitting
	a) Áno
	b) Nie
	
24. Linear regression errors values has to be normally distributed.
	a) Áno
	b) Nie
	
25. Suppose that your linear regression model is underfitting the data. Which of following options would you consider?
	a) Add more variables
	b) Try higher degree of polynomial regression
	c) Do feature selection
	
26. Assign to the correct group: (exploration phase / data collection / the quest / data modelling / model deployment)
	a) finds optimal solution in an area
	b) investigate various promising areas in a search space
	
27. Which of the following are phases of Genetic algorithm?
	a) mutation
	b) selection
	c) fitness function
	d) initial population
	e) crossover
	
28. Can Local Search guarantee to find the best solution in any search space?
	a) Áno
	b) Nie
	
29. What are termination conditions of Genetic algorithms
	a) when the optimal solution is found
	b) the objective function value has reached a certain predefined value
	c) an absolute number of generation is reached
	d) when there is no child of any individuals
	e) no improvement in the population for X iterations
	
30. Assign to the correct group for the Spam filtering agent example. (Sensors / Actuators / Environment / Performance measure)
	a) emails across users, email traffic
	b) mark as spam, transfer messages
	c) email client or email server
	d) spam block, false positives, false negatives
	
31. Assign to the correct group: (Monte Carlo simulation / Imperfect, real-time decisions) - Monte Carlo: , Imperfect:
	a) involve thousands or tens of thousands of recalculations before it is complete
	b) assesses the impact of risk, allowing for better decision making uder certainty
	c) presents all the possible outcomes of decisions
	d) apply a heuristic evaluation function to states in the search
	e) replace the terminal test by a cut-off test that decides when to apply the evaluation function
	
32. Can Grid Search guarantee to find the best solution in any search space?
	a) Áno
	b) Nie
	
33. Which of the following is a classification metric?
	a) R2
	b) RMSE
	c) MAE
	d) Accuracy
	
34. What is support in a multiclass classification report?
	a) the number of classes
	b) the number of labels
	c) the number of true instances for each label
	
35. Vyberte správnu možnosť:
	(TN, FP, FN, TP) = (90, 1, 8, 1)
	Calculate accuracy
	a) another values
	b) 0.80
	c) 0.91
	d) 0.70
	e) 0.90
	
36. Assign to the correct group: (R2, MSE, MAE)
	a)  1/n Σ (y_i - ŷ_i)^2
	b)  1/n Σ |y_i - ŷ_i|
	c)  1 - (Σ (y_i - ŷ_i)^2) / (Σ (y_i - ȳ)^2)

37. Čo je overfitting?
	a) ML model má horšiu kvalitu keď pracuje s novými dátami
	b) ML model sa naučí z trenovacích dát do detailov
	c) ML nemá dostatok dát na tréning
	d) ML model má lepšiu kvalitu pri predikcií ako pri tréningu
	
38. How do machine Learning algorithms learn?
	a) using test and validation data
	b) explicitly from the code
	c) by combination of code and predefined rules
	d) from training data for machine learning
	
39. How to avoid overfitting and underfitting? (overfitting: , underfitting:)
	a) regularization
	b) removing non-informative features
	c) increasing the training time of the model
	d) ensembling weaker models
	e) early stopping the training
	f) cross-validation
	g) increasing the number of features
	h) training with more data
	
40. Vyberte správnu možnosť:
	y_true = [0, 1, 0, 0, 1, 1, 0, 1, 0, 1]
	y_pred = [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]
	Calculate (TN, FP, FN, TN)
	a) (1,2,3,4)
	b) another value
	c) (4,1,2,3)
	d) (4,3,2,1)
	e) (1,3,2,4)
	
41. Vyberte správnu možnosť:
	(TN, FP, FN, TP) = (4,1,2,3)
	Calculate sensitivity
	a) 0.80
	b) 0.70
	c) 0.60
	d) 0.20
	e) another values
	f) 0.75
	
42. List the basic data types
	a) numerical and text
	b) numerical, categorical and text
	c) only numerical values and relevant for machine learning
	d) numerical, categorical, ordinal, text, time series and multimedia
	
43. Vyberte správnu možnosť:
	speed = [86, 111, 86, 103, 69, 87, 78, 77, 86, 65]
	Calculate variance
	a) 84.60
	b) another values
	c) 177.56
	d) 13.32
	e) 86.00 counts=3
	f) -0.45
	g) 86.00
	h) 0.48
	
44. What are the basic tools of Exploratory Data Analysis (EDA)?
	a) plots, graphs, summary statistics
	b) mindset
	c) python
	d) connection with new data
	
45. Which of the following implies no relationship with respect to correlation?
	a) Corr(X, Y) = -1.05
	b) Corr(X, Y) = 0
	c) Corr(X, Y) = 1
	
46. If Corr(X, Y) = 0. Is it right to conclude that X and Y do not have any relation between them?
	a) Áno
	b) Nie
	
47. What has to be done in Exploratory Data Analysis (EDA)?
	a) we find the trend, distribution, average, median, mode, outliers, variance, deviations, correlations
	b) we test hypotheses, do a visual exploration
	c) we check if the data has a suitable structure
	
48. Assign the chart types to the correct group: (univariate analysis / bivariate analysis)
	a) Histogram
	b) Distributions plots
	c) Heatmap
	d) Scatterplot
	e) Boxplot
	f) Regplot
	g) Pairplot
	
49. To avoid data leakage, data preparation must be fit only on  \__________
	a) the test dataset
	b) the whole dataset
	c) the training dataset
	d) the validation dataset
	
50. Vyberte správnu možnosť:
	dfa = pandas.DataFrame({'key': ['k0', 'k1'], 'vA': ['a0', 'a1']})
	dfb = pandas.DataFrame({'key': ['k0'], 'vB': ['b0']})
	Calculate merge(dfa, dfb, on='key', how='left')
	a) index key vA vB | 0 k0 b0 | 1 k1 a1 NaN
	b) index key vA vB | 0 k0 a0 a1
	c) index key vA vB | 0 k1 a1 b1
	d) index key vA vB | 0 k0 a0 b0
	
51. Vyberte správnu možnosť:
	dfa = pandas.DataFrame({'key': ['k0', 'k1'], 'vA': ['a0', 'a1']})
	dfb = pandas.DataFrame({'key': ['k0'], 'vB': ['b0']})
	Calculate dfa.join(dfb, how='left', lsuffix='_A', rsuffix='_B')
	a) index key_A vA key_B vB | 0 k0 a0 k0 b0
	b) index key_A vA key_B vB | 0 k0 a1 k0 b0
	c) index key_A vA key_B vB | 0 k1 a1 k1 a1
	d) index key_A vA key_B vB | 1 k1 a1 NaN NaN
	
52.  Vyberte správnu možnosť:
	dfa = pandas.DataFrame({'key': ['k0', 'k1'], 'vA': ['a0', 'a1']})
	dfb = pandas.DataFrame({'key': ['k0'], 'vB': ['b0']})
	Calculate dfa.join(dfb, how='right', lsuffix='_A', rsuffix='_B')
	a) index key_A vA key_B vB | 0 k0 a0 k0 b0
	b) index key_A vA key_B vB | 0 k0 a1 k0 b0
	c) index key_A vA key_B vB | 0 k1 a1 k1 a1
	d) index key_A vA key_B vB | 1 k1 a1 NaN NaN
	
53. Which if the following is used to extract data from HTML code of websites?
	a) web scraping
	b) web wrangling
	c) web integration
	d) web cleaning
	
54. Vyberte správnu možnosť:
	dfa = pandas.DataFrame({'key': ['k0', 'k1'], 'vA': ['a0', 'a1']})
	dfb = pandas.DataFrame({'key': ['k0'], 'vB': ['b0']})
	Calculate merge(dfa, dfb, on='key', how='outer')
	a) index vA vB | 0 k0 a0 a1
	b) index vA vB | 0 k0 a0 b0
	c) index vA vB | 0 k1 a1 b1
	d) index vA vB | 0 k0 a0 b0 | 1 k1 a1 NaN
	
55. Vyberte správnu možnosť:
	dfa = pandas.DataFrame({'key': ['k0', 'k1'], 'vA': ['a0', 'a1']})
	dfb = pandas.DataFrame({'key': ['k0'], 'vB': ['b0']})
	Calculate dfa.join(dfb, how='inner', lsuffix='_A', rsuffix='_B')
	a) index key_A vA key_B vB | 0 k0 a0 k0 b0
	b) index key_A vA key_B vB | 0 k0 a1 k0 b0
	c) index key_A vA key_B vB | 0 k1 a1 k1 a1
	d) index key_A vA key_B vB | 1 k1 a1 NaN NaN
	
56. Which of the following is the SECOND most important thing in Data Science?
	a) the data (data understanding)
	b) code
	c) the quest (business understanding)
	d) the answer
	
57. Building data model is
	a) a quest with many possible solutions
	b) clear according to software engineering procedures
	c) possible only for data with adequate quality
	d) suitable for ubiquitous data as we have now
	
58. Which of the following is the TOP most important thing in Data Science?
	a) the data (data understanding)
	b) code
	c) the quest (business understanding)
	d) the answer
	
59. CRISP-DM is
	a) an iterative process
	b) a cycle, which spins once, then goes to production and finishes
	
60. Which is included in data preparation step of CRISP-DM?
	a) data wrangling, EDA, feature selection, dimensionality reduction, feature transformation
	b) EDA for visualization and EDA for interface
	c) data understanding, data transformation
	
61. The role of a machine learning engineer is
	a) to define the ideal dataset for machine learning, and to formulate questions and hypotheses
	b) to identify and to implement appropriate types of analysis
	c) to interpret and to communicate the building data pipeline for the data product development
	d) only to analyze and select suitable machine learning algorithms for the machine learning data, which are delivered to him
	
62. CRISP-DM
	a) contains 6 steps: business understanding, data understanding, data preparation, modeling, evaluation and production
	b) has two phases: development and deployment
	
63. Vyberte správnu možnosť:
	A sample S consist of 14 observations, where
	9 observations belong to class c1 and 5 belong to class c2.
	The attribute A of S has two values {a1, a2}.
	For value a1, 6 observations belongs to class c1 and 2 belongs to c2.
	For value a2, 3 observations belongs to c1 and 3 to c2.
	Calculate entropy H(S)
	a) 1.000
	b) 0.811
	c) 0.940
	d) 0.970
	e) 0.048
	f) another value
	
64. A sample S consists of 14 observations, where 9 observations belong to class c1 and 6 observations belong to c2. Calculate Gini(S)
	a) 0.970
	b) 0.048
	c) 0.459
	d) 0.940
	e) 0.480
	f) another value
	
65. Choose from the following which are Decision Tree Nodes:
	a) Splitting
	b) Decision Node
	c) Root Node
	d) Leaf
	
66. Which of the following algorithms are not ensemble learning?
	a) Decision Tree
	b) Random Forest
	c) Adaboost
	
67. Decision tree can be used for
	a) Classification and Regression
	b) Splitting data
	c) Classification
	d) Visualization
	e) Regression
	
68. Vyberte správnu možnosť:
	A sample S consist of 14 observations, where
	9 observations belong to class c1 and 5 belong to class c2.
	The attribute A of S has two values {a1, a2}.
	For value a1, 6 observations belongs to class c1 and 2 belongs to c2.
	We calculated H(S) = 9.40, H(a1) = 0.811, H(a2) = 1.0 in advance.
	Calculate information gain IG(S, A)
	a) 0.940
	b) 0.480
	c) 0.811
	d) another value
	e) 0.970
	f) 0.048
	g) 1.000
	
69. Vyberte správnu možnosť:
	A sample S consist of 14 observations, where
	9 observations belong to class c1 and 5 belong to class c2.
	The attribute A of S has two values {a1, a2}.
	For value a1, 6 observations belongs to class c1 and 2 belongs to c2.
	Calculate entropy H(a1)
	a) 0.940
	b) 0.480
	c) 0.811
	d) another value
	e) 0.970
	f) 0.048
	g) 1.000
	
70. What is an outlier?
	a) data corruption
	b) outlier is rare, distinct, or do not fit in some way
	c) observation that is unlike other observations
	d) measurement error or input error
	e) special kind of sensor data
	
71. Priradte do správnej skupiny (imputation / outlier detection)
	a) Assigning a unique value
	b) KNN Imputer
	c) 1.5 of IQR below 25% or above 75%
	d) Delete rows with missing data
	e) Predicting the missing values
	f) DBScan Clustering
	g) Isolation Forest
	h) Mean Imputation
	i) 3x standard deviations from the mean
	
72. Priradte do správnej skupiny (Dimensional reduction / Feature selection)
	a) Linear Discriminant Analysis LDA
	b) Mutual Information MI
	c) Recursive Feature Elimination RFE
	d) Singular Value Decomposition SVD
	e) MultiDimensional Scaling MDS
	f) Elastic Net
	g) t-Stochastic Neighbor Embedding t-SNE
	h) Variance Threshold
	i) Chi-Square
	j) Isomap
	k) F-value
	l) Selecting of input features that are most relevant to the target variable
	m) Embedded
	n) Principal Components Analysis PCA
	
73. Priradte do správnej skupiny (Make data more Gaussian / Scale numerical data / Encode categorical data) - Gaussian: , Scale: , Encode:
	a) Yeo-Johnson transform
	b) MinMaxScaler
	c) Label encoding
	d) Power transform
	e) Target encoding
	f) Hashing
	g) StandardScaler
	h) RocustScaler
	i) Box-Cox transform
	j) z-normalization
	k) Quantile transform
	l) One Hot Encoding
	
74. In statistics, a Type II error (false negative) occurs when:
	a) H0 is False but decision is "do not reject H0"
	b) H0 is True but decision is "do not reject H0"
	
75. Which of the following fields uses a relatively small amount of data to estimate a bigger population? 
	a) Inferential 
	b) Descriptive
	c) Exploratory
	
76. Which of the following is a good train dataset characteristic?
	a) It is representative of the dataset as a whole
	b) It is small enough for experiments with data
	c) It is large enough to yield meaningful results
	
77. Assign to the correct group: (Gaussian / not normal distribution)
	a) parametric tests
	b) ANOVA
	c) Mann-Whitney U Test
	d) non-parametric tests
	e) Student t-test
	
78. Main assumptions of statistical tests are:
	a) normal distribution / gaussian distribution
	b) data observations are dependent
	c) similar variance
	d) data observations are independent
	e) heteroscedasticity
	
79. Assumptions of Mann-Whitney U Test
	a) samples are not normally distributed
	b) samples are normally distributed
	c) samples and observations must be independent
	d) the number of observations is at least 20 in each sample
	e) the number of samples is 2
	
80. Assign to the correct group (Population / Sample)
	a) all possible observations from the investigation domain
	b) a randomly selected set of observations from the population
	
81. What is basic idea of holdout validation?
	a) to use k different portions (k>2) of the data to train and test a model in a round-robin fashion
	b) to partition the data into a train and test datasets. Then the model is fit on the train and evaluated on a test dataset.
	c) to partition the data into a train, validation and test datasets. Then the model is fit on the train and evaluated on a test dataset.
	
82. A couple has 2 kids and one of them is a girl. What is the probability that the other child is also a girl?
	a) 0.50
	b) 0.75
	c) another
	d) 0.66
	e) 0.33
	f) 0.25
	g) 1.00
	
83. Since 2010, deep learning success is believed due to:
	a) availability of huge amount of data to train
	b) neural networks were introduced
	c) increase of computing power
	d) new algorithmic advantages
	e) deep neural networks were introduced
	f) success of the handwritten digit recognition with LeNet
	
84. Gradient descent is an iterative first-order optimization algorithm used to find a local optimum of a given function. It is commonly used in ML/DL to minimize a loss function.
	a) Áno
	b) Nie
	
85. Assign to the correct groups (Gradient boosting / Gradient descent)
	a) an iterative first-order optimization algorithm used to find a local optimum of a given function. It is commonly used in ML/DL to minimize a loss function.
	b) a type of ML algorithm that is used to improve the accuracy of a ML model. It works by training a series of weak models, then combining their predictions to create a final prediction.
	
86. Deep learning makes profit of using
	a) Field programmable gate array
	b) specialized HW present in accelerated computing environments
	c) Google TPU
	d) GPU as general purpose processors
	
87. Assign to the correct group (Dropout / Batch normalization)
	a) dropping out hidden and visible nodes of a network randomly to prevent overfitting, typically 20% of nodes
	b) normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one
	
88. Assign to the correct group (parameter / hyper-parameter)
	a) configuration variable that is internal to the model whose value can be estimated from data
	b) parameter whose value is set before the learning process begins. It determines how the model is trained and the model structure
	
89. One-Vs-One (OVO) method is
	a) we need to fit (n-1) models to classify into n classes
	b) we need to fit n models in n-class classification problem
	c) we need to fit n(n-1)/2 models to classify into n classes
	
90. One-Vs-Rest (OVR) method is
	a) we need to git only 1 model to classify into n classes
	b) we need to fit (n-1) models to classify into n classes
	c) we need to fit n models in n-class classification problem
	
91.  LASSO can be used for variable selection in Linear Regression
	a) Áno
	b) Nie
	
92. Assign to the correct group (metrika na vybudovanie regresívnych modelov / metrika na hodnotenie regresívnych modelov) - Hodnotenie: , Vybudovanie:
	a) Residual Sum of Subsets (RSS)
	b) Ordinary Least Squares regression (OLS)
	c) Residual Standard Error (RSE)
	
93. Standardization of features is always required before training a Logistic Regression model.
	a) Áno
	b) Nie
	
94. Data bucketing is
	a) used to reduce the effects of minor observation errors
	b) data cleaning
	c) data compression
	d) data discretization
	e) used to transform numeric data to categorical data
	f) data reduction
	g) data binning
	
95. Assign to the correct group (Feature projection / Feature selection) - Selection: , Projection:
	a) reduces the number of input variables for a dataset
	b) selects a subset of input features that are most relevant to the target variable
	
96. Fairness in responsible AI is about
	a) continuous data modeling in development
	b) continuous model and data curation to detect imbalanced outcomes
	c) active monitoring for performance drift and bias review
	d) model retraining
	
97. Assign to group (Responsible AI / Ethics and AI / Trustworthy AI / Human-Centric AI)
	a) Responsible development of human-centric and trustworthy AI systems
	b) methodology for the implementation of AI methods in real organizations
	c) The purpose of AI is to augment Human Intelligence
	d) the only way to mitigate AI risks
	e) a standard for ensuring that AI is safe, trustworthy and unbiased
	
98. Responsible Data Science aims to
	a) make data according to FAIR and FACT
	b) make data present everywhere and for everyone
	c) limiting the potential for misuse that could erode fundamental rights and undermine the public trust in digital technologies
	d) place work in the context of social, legal, ethical aspects
	e) to maximize the availability of high quality data
	
99. Assign to the correct group for Data Governance (Appropriate Usage / Data Lifecycle / Accessibilities / Ownership)
	a) defines how to manage acquiring, storing, selling and purging data
	b) defines who can access the data, what exactly can they see, and under what circumstances
	c) defines what constitutes appropriate and inappropriate use of data internally and externally, particularly for automated decision
	d) defines who owns the data and makes decisions on how and if it can be accessed
	
100. What is DGPR?
	a) the EU's primary data governance regulation applies to any business using data from EU data subjects
	b) inconvenience for everybody because of non-stop signing agreements
	c) the General Data Protection Regulation, according to which EU can fines of up to €20 million or 4% of global revenue
	d) EU law for big companies 
	
101. Select GDPR in simple terms
	a) Public organization, private organization, world-wide data flows regulation
	b) Penalty, Breach, Fine, Global business
	c) Consent, Contract, Legal obligation, Vital interests, Public task, Legitimate interests
	
102. What are the basic tools of Exploratory Data Analysis EDA?
	a) mindset
	b) python
	c) connection with new data
	d) plots, graphs, summary statistics 
	
103. Join the pairs. (skewness > 0 / skewness = 0 / skewness < 0) - a) znamienko 0
	a) normally distributed
	b) more weight in the right tail of the distribution
	c) more weight in the left tail of the distribution
	
104. Vyberte správnu možnosť:
	a = numpy.array([
	[30, 60, 70],
	[70, 90, 10],
	[50, 90, 40]])
	Calculate numpy.amax(a)
	a) 60
	b) [50., 80., 40.]
	c) [70, 90, 90]
	d) [60., 70., 50.]
	e) [30, 60, 10]
	f) [53.33, 56.66, 60.]
	g) 90
	h) 10
	i) 56.66
	j) [70, 90, 70]
	k) [50., 90., 40.]
	
105. Vyberte správnu možnosť:
	a = numpy.array([
	[30, 60, 70],
	[70, 90, 10],
	[50, 90, 40]])
	Calculate numpy.median(a, axis = 0)
	a) 60
	b) [50., 80., 40.]
	c) [70, 90, 90]
	d) [60., 70., 50.]
	e) [30, 60, 10]
	f) [53.33, 56.66, 60.]
	g) 90
	h) 10
	i) 56.66
	j) [70, 90, 70]
	k) [50., 90., 40.]
	
106. What are the Measures of Dispersion?
	a) Mean, Median, Mode, Variance, Standard Deviation
	b) Mean, Median, Mode
	c) Variance, Standard Deviation
	d) Skewness, Kurtosis
	
107. Vyberte správnu možnosť:
	a = numpy.array([
	[30, 60, 70],
	[70, 90, 10],
	[50, 90, 40]])
	Calculate numpy.amin(a, axis=0)
	a) 60
	b) [50., 80., 40.]
	c) [70, 90, 90]
	d) [60., 70., 50.]
	e) [30, 60, 10]
	f) [53.33, 56.66, 60.]
	g) 90
	h) 10
	i) 56.66
	j) [70, 90, 70]
	k) [50., 90., 40.]
	
108. Vyberte správnu možnosť:
	speed = [70, 100, 80, 90, 60, 80, 70, 100, 60, 80]
	Calculate mean
	a) 70
	b) another value
	c) 13.74
	d) 79
	e) 80
	f) 189
	
109. 3V's are not sufficient to describe Big Data.
	a) Áno
	b) Nie
	
110. Vyberte správnu možnosť:
	dfa = pandas.DataFrame({'key': ['k0', 'k1'], 'vA': ['a0', 'a1']})
	dfb = pandas.DataFrame({'key': ['k0'], 'vB': ['b0']})
	Calculate merge(dfa, dfb, on='key', how='inner')
	a) index vA vB | 0 k0 a0 a1
	b) index vA vB | 0 k0 a0 b0
	c) index vA vB | 0 k1 a1 b1
	d) index vA vB | 0 k0 a0 b0 | 1 k1 a1 NaN
	
111. Why is data preprocessing difficult to generalize?
	a) because we can't read all kinds of data in various formats
	b) because the specific data at the core of each ML project is different
	c) because data and model are diverse
	d) because data are not always accessible
	e) because of the philosophy of data preparation is to discover hot to best expose the underlaying structure of the problem to the learning algorithms
	
112. What is the difference between a Validation and a Test dataset?
	a) Validation dataset is a part of the Train dataset and it is used for cross-validation. Test dataset is used to evaluate the performance of the trained model
	b) Validation dataset is a part of the Train dataset and it is used to fit the parameters.  Test dataset is used to evaluate the performance of the train model
	c) Validation dataset is a part of the Train dataset and it is used to avoid overfitting. Test dataset is used to evaluate the performance of the trained model
	
113. Vyberte správnu možnosť:
	dfa = pandas.DataFrame({'key': ['k0', 'k1'], 'vA': ['a0', 'a1']})
	dfb = pandas.DataFrame({'key': ['k0'], 'vB': ['b0']})
	Calculate merge(dfa, dfb, on='key', how='right')
	a) index key vA vB | 0 k0 b0 | 1 k1 a1 NaN
	b) index key vA vB | 0 k0 a0 a1
	c) index key vA vB | 0 k1 a1 b1
	d) index key vA vB | 0 k0 a0 b0
	
114. Raw data should ne processed one time per interval in the deployment phase for machine learning.
	a) Áno
	b) Nie
	
115. Assign to the correct group (Human-Inspired / Swarm-Inspired / Evolution-Inspired / Physics-Inspired)
	a) Hill Climbing
	b) Ant Colony Optimization (ACO)
	c) Simulated Annealing (SA)
	d) Artificial Bee Colony (ABC)
	e) Genetic Algorithm (GA)
	f) Differential Evolution (DE)
	g) Particle Swarm Optimization (PSO)
	h) Firework Algorithm
	i) Harmony Search
	
116. Each agent of swarm intelligence is independent and isolated from other agents
	a) Áno
	b) Nie
	
117. In NLP, the process of identifying people, an organization from a given sentence or paragraph is called:
	a) Lemmatization
	b) Stop word removal
	c) Named entity recognition
	d) Stemming
	
118. Assign to the correct group (Inverse Document Frequency / Term Frequency) - Document: , Term: 
	a) (word[i] frequency in document[j]) / (total words in document[j])
	b) -log2(documents with word[i] / total documents)
	
119. Supervised learning requires a label for each training sample.
	a) Áno
	b) Nie
	
120. What is regularization in machine learning?
	a) adding noisy information in order to prevent overfitting
	b) combination of code and predefined rules c) to increase model complexity
	d) specify the learning procedures
	e) to increase the accuracy of the model
	f) adding a noise to the model to make it more stable
	g) adding a noise to the data to make it more private
	h) adding a penalty term to a model to induce smoothness in order to prevent overfitting
	
121. (TN, FP, FN, TP) = (3,1,2,4). Calculate recall
	a) 0.33
	b) 0.95
	c) 0.80
	d) 0.25
	e) another value
	f) 0.70
	g) 0.66
	h) 0.75
	i) 0.18
	
122. Assign to the correct group (F1 / Accuracy / Recall / Precision)
	a) TP/(TP+FP)
	b) (TP+TN)/(TP+TN+FP+FN)
	c) TP/(TP+FN)
	d) 2\*(Recall\*Precision)/(Recall+Precision)
	
123. Which of the following methods is used to create a model in scikit-learn?
	a) Fit and Predict
	b) Fit and Transform
	c) Predict
	d) Fit
	e) Transform
	
124. (TN, FP, FN, TP) = (3,1,2,4). Calculate precision
	a) 0.75
	b) 0.95
	c) 0.47
	d) 0.25
	e) 0.80
	f) another value
	g) 0.18
	h) 0.70
	i) 0.66
	j) 0.33
	
125. Overfitting is more likely when you have huge amount of data to train
	a) Áno
	b) Nie
	
126. Select the best option to describe a cloud machine
	a) encapsulated environment
	b) virtual environment
	b) isolated environment
	
127. How can you view the first 10 records in a Pandas DataFrame?
	a) first(10)
	b) get(10)
	c) head(10)
	d) top(10)
	
128. In Pandas series, data can be accessed through different functions such as:
	a) iloc()
	b) loc()
	c) get()
	d) access()
	
129. Interpret the p-value
	a) the p-value is the evidence against a H0
	b) the smaller the p-value, the stronger the evident to reject H0
	c) the smaller the p-value, the stronger the evident to reject H1
	d) the p-value is the evidence against a H1
	
130. Assign to the correct group (Descriptive statistics / Inferential statistics)
	a) multivariate correlations
	b) hypothesis testing
	c) summarizing raw observations into information that we can understand
	d) normal distribution testing
	e) central limit theorem
	f) bivariate correlations
	g) applied to means testing
	h) univariate shape, center, spred
	
131. Assign to the correct group (Data cleaning / Data selection / Data preparation)
	a) Outlier detection
	b) Data Sampling
	c) Imputation
	d) Data corruptions, errors, lost
	e) Encoding
	f) Scaling
	g) Transformation
	h) Feature Selection
	
132. What are the properties of an ideal Gaussuan Distribution?
	a) Mean, mode and median are all located in the center
	b) Symmetrical
	c) Unimodal
	d) Bell-shaped
	e) Asymptotic 
	
133. Decision Trees can be used for Classification Tasks
	a) Áno
	b) Nie
	
134. Which of the following are the advantages of Decision Trees?
	a) Good black box model
	b) Can be visualized
	c) Simple to understand and to interpret
	d) White box model
	
135. How to avoid overfitting in Decision Trees?
	a) Pre-prunning
	b) Over-prunning
	c) Post-prunning
	
136. Each decision tree can be interpreted as a set of rules IF-THEN
	a) Áno
	b) Nie
	
137. Which of the following are the disadvantages of Decision Trees?
	a) Overfitting
	b) Biased trees if some classes dominate
	c) White box model
	
138. What are the basic ingredients of random forests?
	a) Randomization and no prunning
	b) Bagging
	c) Prunning
	d) Combination
	
139. Gradient descent is a type of ML algorithm that is used to improve the accuracy of a ML model. It works by training a series of weak models, then combining their predictions to create a final prediction.
	a) Áno
	b) Nie
	
140. Gradient boosting is a type of ML algorithm that is used to improve the accuracy of a ML model. It works by training a series of weak models, then combining their predictions to create a final prediction.
	a) Áno
	b) Nie
	
141. Assign to the correct group (Post-Prunning / Pre-Prunning)
	a) Allow the tree to overfit the training data, then remove branches from a fully grown tree
	b) Stop growing the tree earlier before it reaches the point where it perfectly classifies the training data
	
142. The right node in a decision tree is an attribute with?
	a) high entropy and IG
	b)balanced IG
	c) lowest IG
	d) highest IG
	e) high entropy
	
143. Assumptions of Kruskal-Wallis H Test
	a) samples must be independent and observations must be independent
	b) samples are not normally distributed
	c) the number of samples is only 2
	d) samples are normally distributed
	
144. Assign to the correct group. (Wrapper Feature Selection Methods / Filter Feature Selection Methods / Embedded Feature Selection Methods )
	a) F-value
	b) Forward Selection
	c) Variance Threshold
	d) Selection from Model
	e) Information Gain
	f) Mutual Information
	g) Backward Selection
	h) Ridge (L2)
	i) Lasso (L1)
	j) Chi-Squared
	k) Recursive Feature Elimination
	l) Elastic Net
	
145. Assign to the correct group (Single-solution-based / Population-based)
	a) Particle Swarm Optimization (PSO)
	b) Simulated Annealing
	
146. Data Science is
	a) interdisciplinary field that includes the processes of capturing data, analyzing, and deriving insights from it
	b) data mining
	c) the 4th paradigm called data-intensive scientific discovery
	d) machine learning
	
147. Responsible AI is
	a) the methodology for communication with AI
	b) the only way to mitigate AI risks
	c) the way to check if AI responses
	d) a standard for ensuring that AI is safe, trustworthy and unbiased
	
148. What is MLOps?
	a) software development methodology built on DevOps to deploy and maintain ML models
	b) DevOps for automation, collaboration, integration and configuration
	c) to deploy and maintain ML models of production reliably and efficiently
	d) machine learning
	
149. To avoid data leakage, the entire modeling pipeline must be prepared only on
	a) the training dataset
	b) the validation dataset
	c) the whole dataset
	d) the test dataset
	
150. What is data annotation?
	a) model description
	b) labeling data in preparation for training a supervised ML model
	c) metadata
	d) data labeling
	
151. How does cross-validation work?
	a) The basic idea is to partition the data into a train, validation and test datasets
	b) The basic idea is to partition the data into a train and test datasets
	c) it shuffles data, then splits data into k groups.
	d) It holds out one group for a test and trains a model on the rest.
	e) The evaluation is scored in a round-robin fashion.
	f) it uses k different portions (k>2) of the data to train and test a model at different iterations
	
152. Trade-off between Exploitation and Exploration.
	a) the search for new sources of relevant page
	b) crawling of pages where the expected  value can be predicted with a high confidence
	
153. Vyberte správnu možnosť:
	speed = [70, 100, 80, 90, 60, 80, 70, 100, 60, 80]
	Calculate mode
	a) 70
	b) another value
	c) 13.74
	d) 79
	e) 80
	f) 189
	
154. Vyberte správnu možnosť:
	a = numpy.array([
	[30, 60, 70],
	[70, 90, 10],
	[50, 90, 40]])
	Calculate numpy.amin(a)
	a) 60
	b) [50., 80., 40.]
	c) [70, 90, 90]
	d) [60., 70., 50.]
	e) [30, 60, 10]
	f) [53.33, 56.66, 60.]
	g) 90
	h) 10
	i) 56.66
	j) [70, 90, 70]
	k) [50., 90., 40.]
	
155. Vyberte správnu možnosť:
	speed = [70, 100, 80, 90, 60, 80, 70, 100, 60, 80]
	Calculate median
	a) 70
	b) another value
	c) 13.74
	d) 79
	e) 80
	f) 189
	
156. What is bivariate analysis?
	a) descriptive analysis to examine one variable and its relationship to another variable at one time
	b) attempts to understand the difference between two variables at a time
	c) statistical significance testing for pair of all variables
	
157. Vyberte správnu možnosť:
	a = numpy.array([
	[30, 60, 70],
	[70, 90, 10],
	[50, 90, 40]])
	Calculate numpy.amax(a axis=1)
	a) 60
	b) [50., 80., 40.]
	c) [70, 90, 90]
	d) [60., 70., 50.]
	e) [30, 60, 10]
	f) [53.33, 56.66, 60.]
	g) 90
	h) 10
	i) 56.66
	j) [70, 90, 70]
	k) [50., 90., 40.]
	
158. Vyberte správnu možnosť:
	a = numpy.array([
	[30, 60, 70],
	[70, 90, 10],
	[50, 90, 40]])
	Calculate numpy.median(a, axis=0)
	a) 60
	b) [50., 80., 40.]
	c) [70, 90, 90]
	d) [60., 70., 50.]
	e) [30, 60, 10]
	f) [53.33, 56.66, 60.]
	g) 90
	h) 10
	i) 56.66
	j) [70, 90, 70]
	k) [50., 90., 40.]
	
159. What will the code print?
	arr2d = numpy.arange(0, 30)
	arr2d = arr2d.reshape([3, 10])
	print(arr2d[0,0])
	a) [[0,0],[0,0],[0,0]]
	b) [0,0]
	c) 0
	
160. What does the following code do? df = pandas.read_csv('log_access_file.csv')
	a) Loads a csv file into a DataFrame
	b) Initiates a program in pandas
	c) Ends a program in pandas
	d) Imports pandas directory
	
161. Logistic Regression is mainly used for regression.
	a) Áno
	b) Nie
	
162. Linear Regression is sensitive to outliers
	a) Áno
	b) Nie
	
163. Logistic Regression is mainly used for classification.
	a) Áno
	b) Nie
	
164. Which of the following is true about Residuals ?
	a) Higher is better
	b) Lower is better
	
165. What are the metrics used to test an NLP model?
	a) ROU, AUC
	b) Accuracy, Precision, Recall and F1
	c) MAE, MSE, RMSE
	
166. Which one of the following is not a pre-processing technique in NLP?
	a) Lemmatization
	b) Removal of stop words
	c) Sentiment analysis
	d) Removing punctuations
	e) Converting to lowercase
	f) Stemming
	
167. ML is iterative, typically requiring training more than once
	a) Áno
	b) Nie
	
168. What is machine learning (ML)?
	a) ML is a computer intelligence
	b) ML is a subset of AI techniques that enable computer systems to learn from previous experience and improve their behavior for a given task
	c) ML is artificial intelligence available in computer systems
	
169. What do we get after the learning process?
	a) a model
	b) a dump file with metadata
	c) an approximate function
	d) a black box
	
170. What is underfitting?
	a) refers to high performance on the train data and bad performance on test data
	b) refers to poor performance of the trained model on the training data
	c) refers to failing to learn the problem from the training data sufficiently
	d) refers to high performance on the validation and test data